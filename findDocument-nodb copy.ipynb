{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getText(filePath):\n",
    "    '''Get text from pdf file'''\n",
    "    doc = fitz.open(filePath)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text+=page.get_text()\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    '''Remove stop word'''\n",
    "    stop_words = set(stopwords.words('english')) # Define the set of English stopwords\n",
    "    words = nltk.word_tokenize(text) # Tokenize the input text\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words] # Remove stopwords\n",
    "    return ' '.join(filtered_words) # Join the filtered words into a string\n",
    "\n",
    "def stem_words(text):\n",
    "    '''convert to root word'''\n",
    "    word_tokens = nltk.word_tokenize(text)\n",
    "    stems = [stemmer.stem(word) for word in word_tokens]\n",
    "    return ' '.join(stems)\n",
    "\n",
    "def clean_text(filePath):\n",
    "    text = getText(filePath)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+', '', text) # remove url\n",
    "    # text = ''.join([i for i in text if not i.isdigit()]) #remove number\n",
    "    text = re.sub(r'\\d+', '', text) #remove number\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # remove special character, white space\n",
    "    text = remove_stopwords(text) # remove stop word\n",
    "    text = stem_words(text) #conver words to root words\n",
    "    text = text.encode('ascii', 'ignore').decode() #remove character not ascii\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1  CLASS ACTION SETTLEMENT AGREEMENT AND RELEASE 5.pdf',\n",
       " '1 Approved April 9 2010 Revised April 12 2019 AST Guidelines for.pdf',\n",
       " '100 Philosophy parapsychology and occultism psychology.pdf',\n",
       " '2009_10_13_MWC_STM_Report.pdf',\n",
       " '2023 Congressional Art Competition  Student Information.pdf',\n",
       " '900 History geography and auxiliary disciplines.pdf',\n",
       " 'A CrossLingual Dictionary for English Wikipedia Concepts.pdf',\n",
       " 'A Plan for a.pdf',\n",
       " 'An overview of scientific and scholarly journal publishing.pdf',\n",
       " 'Annex 11 Computerised Systems.pdf',\n",
       " 'Annual Convension 2015 Key Issues emerging out of Panel.pdf',\n",
       " 'AP World History Modern Course and Exam Description Effective.pdf',\n",
       " 'Art  Finance Report 2017 5th edition.pdf',\n",
       " 'ART BUILDING  ART ANNEX AB.pdf',\n",
       " 'Arts Education.pdf',\n",
       " 'Automatically Assessing the Quality of Wikipedia Articles.pdf',\n",
       " 'Brick by Brick Wikipedia and Libraries building on each other.pdf',\n",
       " 'Central bank digital currencies system design and interoperability.pdf',\n",
       " 'Cold Weather Outdoor Play Boosts Immune System Here are Four.pdf',\n",
       " 'Constructed Criteria.pdf',\n",
       " 'convension de palermo.pdf',\n",
       " 'DDC Dewey Decimal Classification  SUMMARIES.pdf',\n",
       " 'DETAILED SYLLABUS FOR THE POST OF HIGHER SECONDARYpdf.pdf',\n",
       " 'Disintermediating your friends How Online Dating in the United.pdf',\n",
       " 'DOCTOR OF PHYLOSOPHY PHD.pdf',\n",
       " 'DOCTOR OF PHYLOSOPHY PHDpdf.pdf',\n",
       " 'Ethnicity and Interests at the 1990 Federated States of Micronesia.pdf',\n",
       " 'EVALUATING INFORMATION THE CORNERSTONE OF CIVIC.pdf',\n",
       " 'Evolution and Link Prediction of the Wikipedia Network.pdf',\n",
       " 'Finance structure and public enlightenment program of the first.pdf',\n",
       " 'First Meeting of Regional Network of Legal And Technical Experts.pdf',\n",
       " 'FOCAL POINT SUMMER 2005 RESILIENCE AND RECOVERY.pdf',\n",
       " 'Foul Tales Public Knowledge Bringing Dantes Divine Comedy to.pdf',\n",
       " 'From Cradle to Cane The C st of Being a Female C nsumer.pdf',\n",
       " 'G20 Chairs Summary and Outcome Document First G20 Finance.pdf',\n",
       " 'Garfield E Citation Indexes for Science A New Dimension in .pdf',\n",
       " 'Garfield E Citation Indexes for Science A New Dimension in.pdf',\n",
       " 'Garfield E The Agony and the Ecstasy  The History and Meaning of.pdf',\n",
       " 'GCSE 91 History  Specification.pdf',\n",
       " \"God's image of you ( PDFDrive ).pdf\",\n",
       " 'GPT4 System Card  OpenAI.pdf',\n",
       " 'GradientBased Learning Applied to Document Recognition.pdf',\n",
       " 'H2020 Programme Guidelines to the Rules on Open Access to .pdf',\n",
       " 'H2020 Programme Guidelines to the Rules on Open Access to.pdf',\n",
       " 'h2020-hi-oa-pilot-guide_en.pdf',\n",
       " 'Handling Divergent Reference Texts when Evaluating TabletoText.pdf',\n",
       " 'History of the Wraparound Process.pdf',\n",
       " 'How to Read a Paper  Handouts.pdf',\n",
       " 'HOW TO WRITE A LITERARY ANALYSIS ESSAY.pdf',\n",
       " 'How to Write a Paper in Scientific Journal Style and Format.pdf',\n",
       " 'How-to-Write-Guide-v10-2014.pdf',\n",
       " 'ICH Topic Q 2 R1 Validation of Analytical Procedures Text and.pdf',\n",
       " 'Images of Evil Images of Kings_ The Contrasting Faces of the Roy.pdf',\n",
       " 'IMM 5483 E  Document Checklist  For a Study Permit.pdf',\n",
       " 'Instructions for Form I9 Employment Eligibility Verification.pdf',\n",
       " 'Intermediality Intertextuality and Remediation A Literary.pdf',\n",
       " 'ISTANBUL DOCUMENT 1999.pdf',\n",
       " 'IWHYM CHOW FLAME OF LOVE.pdf',\n",
       " 'KEVIN M MILLER MD  Los Angeles.pdf',\n",
       " 'Kevin-Miller-CV.pdf',\n",
       " 'Key2Vec Automatic Ranked Keyphrase Extraction from Scientific .pdf',\n",
       " 'Key2Vec Automatic Ranked Keyphrase Extraction from Scientific.pdf',\n",
       " 'Learning a Lexical Simplifier Using Wikipedia.pdf',\n",
       " 'Lewis.pdf',\n",
       " 'Library of Congress Classification Outline Class B  Philosophy.pdf',\n",
       " 'Life of Adam and Eve.pdf',\n",
       " 'LIST OF ACCEPTABLE SUPPORTING DOCUMENTS FOR.pdf',\n",
       " 'Literary History and Translation An Indian View.pdf',\n",
       " 'Literary Lab Pamphlet 11.pdf',\n",
       " 'Literary Lab.pdf',\n",
       " 'LOVE LIFE SATISFACTION AROUND THE WORLD.pdf',\n",
       " 'Making Love Legible in China Politics and Society during the.pdf',\n",
       " 'MALE AND FEMALE HE CREATED THEM.pdf',\n",
       " 'Marie de Frances Courtly Love The Liberation of Women Through.pdf',\n",
       " 'Measuring the Reliability of Qualitative Text Analysis Data.pdf',\n",
       " 'Modie de convension de stage.pdf',\n",
       " 'Mozaic of Phylosophy and Physicis in Tourism with View to Climate.pdf',\n",
       " 'Multimedia50-55.pdf',\n",
       " 'Multimedia_Database_Management_Systems_(Artech House)-20-50.pdf',\n",
       " 'Multimedia_Database_Management_Systems_(Artech House).pdf',\n",
       " 'N18-2100.pdf',\n",
       " 'National Curriculum  Art and design key stages 1 to 2.pdf',\n",
       " 'nature-positive-plan.pdf',\n",
       " 'Negotiated Agreement text initialled by the EU and OACPS chief.pdf',\n",
       " 'On Traumatic Knowledge and Literary Studies.pdf',\n",
       " 'Philosophy  Lehigh Catalog.pdf',\n",
       " 'Philosophy PHIL.pdf',\n",
       " 'Placing a text in context.pdf',\n",
       " 'Public consultation document Pillar One  Amount A Draft.pdf',\n",
       " 'Queer Love  David M Halperin.pdf',\n",
       " 'Questions to Think About while constructing a Teaching Philosophy.pdf',\n",
       " 'QuillBot as an online tool Students alternative in paraphrasing and.pdf',\n",
       " 'QuillBot as an online tool Students alternative in paraphrasing andpdf.pdf',\n",
       " 'Referred customers are more profitable and more loyal.pdf',\n",
       " 'Registered Exporter System REX Guidance document.pdf',\n",
       " 'Research Brief.pdf',\n",
       " 'REVIEW Of Love and Papers How Immigration Policy Affects.pdf',\n",
       " 'sad1109Jaco5p.indd.pdf',\n",
       " 'Sample Academic Reading Summary Completion selecting words.pdf',\n",
       " 'Subject Benchmark Statement Philosophy.pdf',\n",
       " 'Syllabus Cambridge IGCSE History 0470.pdf',\n",
       " 'System of EnvironmentalEconomic Accounting 2012Central.pdf',\n",
       " 'T Community ART Group Toolkit.pdf',\n",
       " 'Text as Data The Promise and Pitfalls of Automatic Content Analysis.pdf',\n",
       " 'The Anatomical Dry EyeA Different Form of Ocular Surface.pdf',\n",
       " 'The Basic Business Philosophy of the Panasonic Group.pdf',\n",
       " 'The Coaching Questions Handbook_ 150 Powerful Questions for Life Coaching and Personal Growth (powerful questions, coaching questions, life coach, life coaching ) (Volume 1) ( PDFDrive ).pdf',\n",
       " 'The Complications of AgeRelated Macular Degeneration.pdf',\n",
       " 'The Gender Earnings Gap in the Gig Economy Evidence from over.pdf',\n",
       " 'The Ontario Curriculum Grades 18 The Arts 2009 revised.pdf',\n",
       " 'Transformers For Markdown Article Rewriting.pdf',\n",
       " 'Transformers For Markdown Article Rewritingpdf.pdf',\n",
       " 'Vienna Convention on Diplomatic Relations 1961.pdf',\n",
       " 'Vienna Convention on the Law of Treaties 1969.pdf',\n",
       " 'VISUAL ARTS  Creating.pdf',\n",
       " 'WCC2016Res069EN Defining Naturebased Solutions.pdf',\n",
       " 'What Does Text Complexity Mean for English Learners and.pdf',\n",
       " 'What is Spatial History.pdf',\n",
       " 'Why We Read Wikipedia.pdf',\n",
       " 'Winnowing Local Algorithms for Document Fingerprinting.pdf',\n",
       " 'Zhou Xiaochuan Reform the international monetary system.pdf']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "listFile = os.listdir('documents')\n",
    "listFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "listText = []\n",
    "for file in listFile:\n",
    "    # path = os.path.join('documents', file)\n",
    "    # print(path)\n",
    "    text = clean_text(os.path.join('documents', file))\n",
    "    listText.append(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = clean_text('Multimedia50-55.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfvec = TfidfVectorizer()\n",
    "X = tfidfvec.fit_transform(listText)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryVec = tfidfvec.transform([query])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03053217, 0.02230474, 0.03087921, 0.07050303, 0.03336728,\n",
       "        0.02131897, 0.03066563, 0.03130259, 0.07050303, 0.10014076,\n",
       "        0.04384613, 0.06419728, 0.03519373, 0.00239299, 0.06079113,\n",
       "        0.09933763, 0.03183094, 0.05503372, 0.01256906, 0.05378003,\n",
       "        0.00274584, 0.03579042, 0.02520821, 0.04232854, 0.0222257 ,\n",
       "        0.0222257 , 0.03482099, 0.08139815, 0.07460772, 0.03627958,\n",
       "        0.00847478, 0.02544231, 0.03798847, 0.03070185, 0.04236948,\n",
       "        0.10522281, 0.10522281, 0.05116392, 0.05285894, 0.03043238,\n",
       "        0.08967053, 0.0346895 , 0.07594876, 0.07594876, 0.07594876,\n",
       "        0.08749575, 0.0246227 , 0.03643701, 0.05278263, 0.10891718,\n",
       "        0.10891718, 0.04602932, 0.05867589, 0.06187553, 0.04925892,\n",
       "        0.07342382, 0.04213997, 0.01027177, 0.01440987, 0.01440987,\n",
       "        0.06029825, 0.06029825, 0.05537146, 0.06706447, 0.00489827,\n",
       "        0.01288668, 0.03891997, 0.01629503, 0.05058909, 0.05614433,\n",
       "        0.01517361, 0.0370552 , 0.06159847, 0.0130131 , 0.06019728,\n",
       "        0.00328788, 0.04231759, 1.        , 0.55313828, 0.62586707,\n",
       "        0.06029825, 0.04359188, 0.04437837, 0.04281964, 0.05499675,\n",
       "        0.02458541, 0.02362219, 0.10688598, 0.06640216, 0.02958992,\n",
       "        0.01762146, 0.04877653, 0.04877653, 0.02534397, 0.04447149,\n",
       "        0.02116214, 0.07860374, 0.03130259, 0.04491127, 0.05683949,\n",
       "        0.04552955, 0.0765717 , 0.01262431, 0.09896231, 0.01429013,\n",
       "        0.0397565 , 0.08416819, 0.03263379, 0.03130332, 0.09560919,\n",
       "        0.04113193, 0.04113193, 0.01923712, 0.01914768, 0.04633219,\n",
       "        0.01246259, 0.07482675, 0.05620391, 0.10135324, 0.05860254,\n",
       "        0.0253528 ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity = cosine_similarity(queryVec, X)\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{0.9999999999999998, 'Multimedia50-55.pdf'},\n",
       " {0.6258670716936332,\n",
       "  'Multimedia_Database_Management_Systems_(Artech House).pdf'},\n",
       " {0.5531382756761338,\n",
       "  'Multimedia_Database_Management_Systems_(Artech House)-20-50.pdf'},\n",
       " {0.10891718314247391,\n",
       "  'How to Write a Paper in Scientific Journal Style and Format.pdf'},\n",
       " {0.10891718314247391, 'How-to-Write-Guide-v10-2014.pdf'},\n",
       " {0.10688598191420823, 'Placing a text in context.pdf'},\n",
       " {0.10522281206892091,\n",
       "  'Garfield E Citation Indexes for Science A New Dimension in .pdf'},\n",
       " {0.10522281206892091,\n",
       "  'Garfield E Citation Indexes for Science A New Dimension in.pdf'},\n",
       " {0.10135324199444724, 'Why We Read Wikipedia.pdf'},\n",
       " {0.10014076442357916, 'Annex 11 Computerised Systems.pdf'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_indices = similarity.argsort()[0][::-1][:10]\n",
    "res = [{listFile[i], similarity[0][i]} for i in similar_indices]\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "countVec = CountVectorizer()\n",
    "X2 = countVec.fit_transform(listText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryVec2 = countVec.transform([query])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05082019, 0.05084523, 0.04608237, 0.08639682, 0.0520884 ,\n",
       "        0.03046109, 0.06803744, 0.05734435, 0.08639682, 0.12577217,\n",
       "        0.09230597, 0.08223198, 0.04737802, 0.00543902, 0.08193817,\n",
       "        0.12234327, 0.05958389, 0.08704198, 0.02545491, 0.07627758,\n",
       "        0.00547452, 0.06084274, 0.03950761, 0.06193417, 0.0316777 ,\n",
       "        0.0316777 , 0.06449152, 0.10245943, 0.12233114, 0.05685766,\n",
       "        0.01384885, 0.04299444, 0.07696803, 0.05725396, 0.06632632,\n",
       "        0.11756723, 0.11756723, 0.0670254 , 0.08170698, 0.04744863,\n",
       "        0.1403118 , 0.07215952, 0.0868921 , 0.0868921 , 0.0868921 ,\n",
       "        0.12183119, 0.05817276, 0.04493053, 0.08545634, 0.1327158 ,\n",
       "        0.1327158 , 0.07148666, 0.08837349, 0.09028543, 0.06634609,\n",
       "        0.11926215, 0.066561  , 0.02254099, 0.02670534, 0.02670534,\n",
       "        0.09712853, 0.09712853, 0.08518395, 0.08883541, 0.0127747 ,\n",
       "        0.02357098, 0.06421224, 0.02918967, 0.0985806 , 0.0736253 ,\n",
       "        0.02588113, 0.0565113 , 0.08484843, 0.02385823, 0.08445963,\n",
       "        0.00603556, 0.07467554, 0.93821243, 0.60639297, 0.57215152,\n",
       "        0.09712853, 0.06021747, 0.06551423, 0.06266197, 0.08999606,\n",
       "        0.04736907, 0.04702867, 0.11753791, 0.10529176, 0.05467009,\n",
       "        0.02595829, 0.07534381, 0.07534381, 0.03576256, 0.07489225,\n",
       "        0.0522496 , 0.11576276, 0.05734435, 0.06271055, 0.0834959 ,\n",
       "        0.07494394, 0.1056773 , 0.03566093, 0.11927055, 0.0274838 ,\n",
       "        0.05397028, 0.09683692, 0.06177782, 0.05449053, 0.11376099,\n",
       "        0.08270541, 0.08270541, 0.0273757 , 0.03183716, 0.0713836 ,\n",
       "        0.02776723, 0.09932722, 0.07706301, 0.1357919 , 0.10562265,\n",
       "        0.04750796]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity2 = cosine_similarity(queryVec, X2)\n",
    "similarity2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{0.9382124299610846, 'Multimedia50-55.pdf'},\n",
       " {0.6063929709223785,\n",
       "  'Multimedia_Database_Management_Systems_(Artech House)-20-50.pdf'},\n",
       " {0.5721515178839358,\n",
       "  'Multimedia_Database_Management_Systems_(Artech House).pdf'},\n",
       " {0.1403118005999232, 'GPT4 System Card  OpenAI.pdf'},\n",
       " {0.13579189803604605, 'Why We Read Wikipedia.pdf'},\n",
       " {0.13271579615139978,\n",
       "  'How to Write a Paper in Scientific Journal Style and Format.pdf'},\n",
       " {0.13271579615139978, 'How-to-Write-Guide-v10-2014.pdf'},\n",
       " {0.12577216670106567, 'Annex 11 Computerised Systems.pdf'},\n",
       " {0.12234326840661322,\n",
       "  'Automatically Assessing the Quality of Wikipedia Articles.pdf'},\n",
       " {0.12233114387285227,\n",
       "  'Evolution and Link Prediction of the Wikipedia Network.pdf'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_indices2 = similarity2.argsort()[0][::-1][:10]\n",
    "res2 = [{listFile[i], similarity2[0][i]} for i in similar_indices2]\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dele\\\\listFile.pickle', 'wb') as file:\n",
    "    pickle.dump(listFile, file)\n",
    "\n",
    "with open('dele\\\\listText.pickle', 'wb') as file:\n",
    "    pickle.dump(listText, file)\n",
    "\n",
    "with open('dele\\\\tfidfvec.pickle', 'wb') as file:\n",
    "    pickle.dump(tfidfvec, file)\n",
    "\n",
    "with open('dele\\\\countVec.pickle', 'wb') as file:\n",
    "    pickle.dump(countVec, file)\n",
    "\n",
    "with open('dele\\\\tfidf-file-to-vec.pickle', 'wb') as file:\n",
    "    pickle.dump(X, file)\n",
    "    \n",
    "with open('dele\\\\count-file-to-vec.pickle', 'wb') as file:\n",
    "    pickle.dump(X2, file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('pythonenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da81743f5e5325665d1b6157bdd4f0c695488c1af8b8fba7eb689df38b91f087"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
